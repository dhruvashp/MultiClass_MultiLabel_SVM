{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE :\n",
    "Test, over initial random cluster. Read markdown at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "The hamming loss vector for all iterations, in %, is : \n",
      "    Hamming Loss\n",
      "1       22.2469\n",
      "2       22.2423\n",
      "3       24.5263\n",
      "4       22.2423\n",
      "5       22.2423\n",
      "6       22.2423\n",
      "7       24.5124\n",
      "8       22.2423\n",
      "9       22.1635\n",
      "10      22.2423\n",
      "The hamming distance vector for all iterations is : \n",
      "    Hamming Distance\n",
      "1          0.667408\n",
      "2          0.667269\n",
      "3          0.735789\n",
      "4          0.667269\n",
      "5          0.667269\n",
      "6          0.667269\n",
      "7          0.735372\n",
      "8          0.667269\n",
      "9          0.664906\n",
      "10         0.667269\n",
      "The hamming score/accuracy score vector for all iterations, in %, is : \n",
      "    Hamming Score\n",
      "1        80.6139\n",
      "2        80.6208\n",
      "3        76.4153\n",
      "4        80.6208\n",
      "5        80.6208\n",
      "6        80.6208\n",
      "7        76.4292\n",
      "8        80.6208\n",
      "9        80.7396\n",
      "10       80.6208\n",
      "k-values are : \n",
      " [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "train = pd.read_csv('Train_Data.csv',index_col = 0)\n",
    "test = pd.read_csv('Test_Data.csv',index_col = 0)\n",
    "whole = pd.concat([train,test],axis=0)\n",
    "whole.reset_index(drop=True,inplace=True)\n",
    "features = whole.drop(columns=['Family','Genus','Species'])\n",
    "label_class = whole.iloc[:,[22,23,24]]\n",
    "                   \n",
    "                   \n",
    "\n",
    "hamming_loss_vector = pd.DataFrame(index=np.arange(1,11),columns=['Hamming Loss'])\n",
    "hamming_distance_vector = pd.DataFrame(index=np.arange(1,11),columns=['Hamming Distance'])\n",
    "hamming_score_vector = pd.DataFrame(index=np.arange(1,11),columns=['Hamming Score'])\n",
    "k_selected_vector = np.zeros(10)\n",
    "\n",
    "for mc in np.arange(0,10):\n",
    "    \n",
    "    print(mc)  # to keep iteration track\n",
    "                   \n",
    "    sil = pd.DataFrame(index=np.arange(2,51),columns=['Silhouette_Score_Average'])\n",
    "    for i in np.arange(2,51):\n",
    "        kmeans = KMeans(n_clusters=i,init='random').fit(features)\n",
    "        pred_clusters = kmeans.labels_\n",
    "        sil.iloc[i-2] = silhouette_score(features,pred_clusters)\n",
    "\n",
    "    max_sil = sil.max(axis=0)\n",
    "    max_indx = np.argmax(sil['Silhouette_Score_Average'].to_numpy().flatten())\n",
    "    k_selected = max_indx + 2\n",
    "    k_selected_vector[mc] = k_selected\n",
    "\n",
    "    kmeans = KMeans(n_clusters = k_selected, init='random').fit(features)\n",
    "    cluster_groups = kmeans.labels_\n",
    "\n",
    "    cluster_triplet_df = pd.DataFrame(index = np.arange(0,k_selected),columns=['Fam_Clus','Gen_Clus','Spec_Clus']) # has cluster label details\n",
    "    for q in np.arange(0,k_selected):\n",
    "        for l in np.arange(0,3):\n",
    "            loc_build = []\n",
    "            for p in np.arange(0,features.shape[0]):\n",
    "                if cluster_groups[p] == q:\n",
    "                    loc_build.append(label_class.iloc[p,l])\n",
    "\n",
    "            build = np.array(loc_build)\n",
    "            build_counts = np.bincount(build)\n",
    "            highest_freq = np.argmax(build_counts)\n",
    "            cluster_triplet_df.iloc[q,l] = highest_freq\n",
    "\n",
    "\n",
    "    cluster_predictions_label_class = pd.DataFrame(index = np.arange(0,features.shape[0]),columns = ['Family','Genus','Species'])\n",
    "\n",
    "    for i in np.arange(0,features.shape[0]):\n",
    "        cluster_predictions_label_class.iloc[i,0] = cluster_triplet_df.iloc[cluster_groups[i],0]\n",
    "        cluster_predictions_label_class.iloc[i,1] = cluster_triplet_df.iloc[cluster_groups[i],1]\n",
    "        cluster_predictions_label_class.iloc[i,2] = cluster_triplet_df.iloc[cluster_groups[i],2]\n",
    "\n",
    "\n",
    "    mis = 0\n",
    "    for i in np.arange(0,features.shape[0]):\n",
    "        for j in np.arange(0,3):\n",
    "            if cluster_predictions_label_class.iloc[i,j] != label_class.iloc[i,j]:\n",
    "                mis = mis + 1\n",
    "\n",
    "\n",
    "    hamming_loss = (mis/(3*features.shape[0]))*100\n",
    "    \n",
    "    hamming_loss_vector.iloc[mc,0] = hamming_loss               \n",
    "\n",
    "    hamming_distance = mis/features.shape[0]\n",
    "    \n",
    "    hamming_distance_vector.iloc[mc,0] = hamming_distance\n",
    "\n",
    "\n",
    "    hs = 0\n",
    "    for i in np.arange(0,features.shape[0]):\n",
    "\n",
    "        sam_loc = 0\n",
    "        for j in np.arange(0,3):\n",
    "\n",
    "            if cluster_predictions_label_class.iloc[i,j] == label_class.iloc[i,j]:  # note the equality here, true calculated\n",
    "                sam_loc = sam_loc + 1\n",
    "\n",
    "        pred_loc = cluster_predictions_label_class.iloc[i,:].to_numpy().flatten()\n",
    "        true_loc = label_class.iloc[i,:].to_numpy().flatten()\n",
    "\n",
    "        uni_rep_strng = np.concatenate((pred_loc,true_loc)).flatten()  # union, absolute, with repetitions\n",
    "\n",
    "        uni_bin = np.bincount(uni_rep_strng.astype(int))\n",
    "\n",
    "        for m in np.arange(0,uni_bin.size):\n",
    "            if uni_bin[m] > 1:\n",
    "                uni_bin[m] = 1\n",
    "\n",
    "        dist_cnt = np.sum(uni_bin)    # count of total distinct elements\n",
    "\n",
    "        hs_loc = sam_loc/dist_cnt\n",
    "\n",
    "        hs = hs + hs_loc          # running sum of HS\n",
    "\n",
    "\n",
    "    hamming_score = (hs/features.shape[0])*100\n",
    "    \n",
    "    hamming_score_vector.iloc[mc,0] = hamming_score\n",
    "                   \n",
    "            \n",
    "\n",
    "print('The hamming loss vector for all iterations, in %, is : \\n',hamming_loss_vector)\n",
    "\n",
    "print('The hamming distance vector for all iterations is : \\n',hamming_distance_vector)\n",
    "                   \n",
    "print('The hamming score/accuracy score vector for all iterations, in %, is : \\n',hamming_score_vector)         \n",
    "\n",
    "print('k-values are : \\n',k_selected_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average hamming loss over 10 iterations is in % : \n",
      " Hamming Loss    22.690294\n",
      "dtype: float64\n",
      "The average hamming distance over 10 iterations is : \n",
      " Hamming Distance    0.680709\n",
      "dtype: float64\n",
      "The average hamming score/accuracy over 10 iterations is in %: \n",
      " Hamming Score    79.792286\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "avg_hamming_loss = hamming_loss_vector.mean(axis=0)\n",
    "avg_hamming_distance = hamming_distance_vector.mean(axis=0)\n",
    "avg_hamming_score = hamming_score_vector.mean(axis=0)\n",
    "\n",
    "print('The average hamming loss over 10 iterations is in % : \\n',avg_hamming_loss)\n",
    "print('The average hamming distance over 10 iterations is : \\n',avg_hamming_distance)\n",
    "print('The average hamming score/accuracy over 10 iterations is in %: \\n',avg_hamming_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a test Monte Carlo case\n",
    "Difference between this and the normal Monte Carlo case is as follows :\n",
    "\n",
    "Here, K-Means, both for selecting optimal 'k' and for final clustering over that optimal 'k', initializes its initial cluster randomly, specified by init='random'\n",
    "\n",
    "This, as specified in the documentation, does take somewhat longer time to converge, but still has been tested here.\n",
    "\n",
    "As such, due to run time constraints, we only performed it with this randomized cluster initiation, for 10 times.\n",
    "\n",
    "As such, there doesn't seem to be much, if at all any difference from the previous cases.\n",
    "\n",
    "Thus we can conclude that the previous case, which initialized with an optimal initial cluster arrangement (chosen internally by k-means) did, possibly converge to the global optimum.\n",
    "\n",
    "The run time for 10 iterations of this random initialized approach was almost half of that it took for 50 iterations with the optimal initial vector setup by k-means in scikit.\n",
    "\n",
    "As such, if run time hadn't been so large, we would've run it also for 50 iterations just like in the previous case\n",
    "\n",
    "But still, this result, somewhat indicates that,\n",
    "\n",
    "k=4\n",
    "HL = 22 %\n",
    "HD = 0.68\n",
    "HS/Accuracy = 80 %\n",
    "\n",
    "are obtained with the clustering done, reaching its global optimum, quite sufficiently, which is what monte carlo wanted us to ensure, that the picked k, optimal, should also cluster such that we reach not a local optimum but a global one.\n",
    "\n",
    "Running this randomized version, 50 or say 100 times, could help us get a better picture, however, of whether our original and current setup has reached the global optimum or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
